[
  {
    "UID": "1",
    "abstract": "The body is amassed with electrical signals. These bioelectric signals are tied\nto both the most basic body functions such as the heart beating all the way to\nthe highest level of processing in the brain. These signals are often rhythmic\nin nature and provide a means to measure the fundamental rhythms of any\nindividual. In this performance we will generate music in real time from\nbioelectric signals that are read from a human subject. The ECG (heart) will set\nthe rhythm of the music while other signals including EEG (brain waves) and EMG\n(muscles) will be used by an algorithm to perform melodic and rhythmic choices.\nThe subject will participate by dancing and responding to the generated music\nand by virtue of their interaction will change the music produced. Additionally,\nthe signals will be sonified and form part of the music. Musician(s) will also\nimprovise along with the generated music with their instruments being modulated\nby the captured bioelectric signals.",
    "authors": "Matthew Walsh",
    "bio": "Matthew Walsh is a musician, software engineer and neuroscientist, obtaining a\nPhD in Computational Neuroscience in 2020. His thesis topic was \u201cMean Field\nModelling of Epileptogenic Cortical Lesions Using The Liley Model\u201d. This thesis\nfocussed on modelling the effects of cortical lesions on the EEG. Matthew has\nnow broadened his research interests to include the sonification and\nmusification of biological signals.\n\nMatthew is vocalist, guitarist and songwriter for the Melbourne band \u201cYour Creepy Ex\u201d.",
    "image": "TRUE",
    "keywords": "",
    "paper_url": "",
    "session_name": "gig",
    "session_position": "",
    "slides": "",
    "title": "The Hyperdimensional Resonator",
    "video": ""
  },
  {
    "UID": "2",
    "abstract": "The sea is a continuing theme throughout Niamh's work. This piece considers the\nmetaphorical link between the internet and the sea, exploring how internet\nartefacts - images, songs, videos, etc- appear to us, washed ashore on the\ncoasts of our phones and laptops pushed by the tides of algorithms. Using sound\nsamples from the deep depths of the internet, dark places unpopulated by \"likes\"\nor \"views\", 'The internet as metaphor for the sea' honours these rare pearls of\nknowledge. The projected visuals of reflective tendrils sway and collide like\nwaves. Building seascapes with Pure Data and rhythmic waves of Tidal Cycle code,\nthis piece aims to pull the viewer into a synthesized aquatic world.",
    "authors": "Niamh McCool",
    "bio": "Niamh is a multi-disciplinary artist currently based in Ngunnal\nCountry/Canberra. Art, whatever the form, has always been Niamh's way to make\nsense of the world. Niamh is all about accessibility of education, in particular\nanything related to tech and creativity. By day, Niamh works as a tutor at ANU,\nteaching creative computing to university and high school students; by night,\nshe co-runs the DJ collective Vessel, organising workshops and events. She\nstrongly believes in shifting computing to have care, empathy and culture at its\ncore. Rumour has it that Niamh is about to launch her own label.",
    "image": "TRUE",
    "keywords": "",
    "paper_url": "",
    "session_name": "gig",
    "session_position": "",
    "slides": "",
    "title": "The internet as metaphor for the Sea",
    "video": ""
  },
  {
    "UID": "3",
    "abstract": "This performance is a musical improvisation on top of a soundscape generated\nusing the System of a Down installation. The System of a Down installation was\ndeveloped as a collaboration between Rodolfo Ocampo, Oliver Bown, Uncanny Valley\nand the School of Cybernetics to showcase the interconnectedness of human,\ntechnological and environmental systems by turning Earth\u2019s data into a\ngenerative soundscape.\n\nThe installation ingests data from Earth\u2019s complex systems, including CO2\nlevels, weather, economic indicators, and passes it as prompts to GPT-3 for it\nto generate descriptions of the data. Via a semantic similarity computation,\nthese descriptions are then matched to sounds in a sound library that was tagged\nby people describing each sound. Therefore, the soundscape is intended to \u2018feel\u2019\nlike the data, by semantically matching AI generated data descriptions of data\nto human generated descriptions of sounds.\n\nRodolfo uses this soundscape as a base for a real time musical improvisation\nthat can be understood as a way to jam with Earth\u2019s data. Throughout history,\nhumans have related to the Earth through stories and music. In a time where we\nincreasingly relate to the world through data, this performance is a playful\nexploration that repurposes data technologies to relate to these systems in a\nless rational, more emotional and improvisational way through music, assuming\noneself very as part of nature\u2019s orchestra.",
    "authors": "Rodolfo Ocampo",
    "bio": "Rodolfo Ocampo is a creative technologist from Mexico City, based in Sydney. He\nis completing a PhD at UNSW School of Art and Design, researching how\ninteraction design can enable effective human-AI creative collaboration. He\noriginally came to Australia in 2020 to participate in the second cohort at the\n3A Institute, now the School of Cybernetics at ANU. Previous to that, he worked\nat Google for two years. Additionally he is currently working at CSIRO\ndeveloping tools for indigenous rangers to aid in management of Country.\n\nHe is interested in the creative intersection between the environment,\ntechnology and people, and how artificial intelligence can enable new creative\npossibilities. He recently co-developed an installation with music technology\ncompany Uncanny Valley for the School of Cybernetics Building, which transforms\ndata from Earth\u2019s complex systems into a soundscape via semantic matching\nbetween data descriptions and sound descriptions. His performance at OzChi uses\nthis installation soundscape as a base to improvise in real time with keyboards,\nguitar and drums.\n\nRecently, he also developed the viral app Narrative Device which enabled people\nto write short stories with GTP-3. Previous to that, he developed a nationally\ntouring art installation in Mexico that repurposed face recognition technologies\nto protest the war on drugs. He works as an independent consultant and developer\nfor creative projects that involve interactions between people, technology and\nthe environment.",
    "image": "TRUE",
    "keywords": "",
    "paper_url": "",
    "session_name": "gig",
    "session_position": "",
    "slides": "",
    "title": "Latent Space",
    "video": ""
  },
  {
    "UID": "4",
    "abstract": "This livecoding set explores natural and synthetic sonic textures programmed in\nthe Extempore livecoding environment.",
    "authors": "Ushini Attanayake|Ben Swift",
    "bio": "#### Ushini Attanayake\n\nUshini Attanayake is a live coder, educator and PhD candidate in the School of\nComputing at the Australian National University. Her research interests lie in\ncomputing education, creative computing and co-creative agents. Her doctoral\nresearch involves developing a notion of proficiency within creative computing\neducation while drawing on existing notions of proficiency in visuals arts,\nmusic and computing. Focussing on late secondary and early tertiary education,\nshe will explore the efficacy of various approaches used to evaluate learning in\ncreative computing. She is passionate about developing creative computing\neducational material, particularly for outreach programs aimed at delivering\ncomputing education to underrepresented demographics.\n\nHer creative practices include live coding performances, which have lately been\nheavily influenced by classical Indian rhythmic structures. As a house dancer\nand music maker, she continues to look for ways to bridge these two interest\nthrough small projects involving wearable devices in her spare time.\n\n#### Ben Swift\n\n[Dr. Ben Swift](https://benswift.me) is an interdisciplinary scholar and artist-programmer with\ninterests in computational art & music, cybernetics, AI/machine learning, data\nvis/data science and human-computer interaction. A unifying thread is the\npotential of liveness (human-in-the-loop interactivity with real-time feedback)\nin tools and workflows, especially in open-ended creative tasks.",
    "image": "TRUE",
    "keywords": "",
    "paper_url": "",
    "session_name": "gig",
    "session_position": "",
    "slides": "",
    "title": "Livecoding in Extempore",
    "video": ""
  },
  {
    "UID": "5",
    "abstract": "Head-mounted mixed reality presents new opportunities for natural musical\ncontrol in three dimensions for facilitating musical creativity, yet existing\nworks are limited to using hand-held controllers. In OzCHI this year, we present\na free-improvised performance consisting of a novel 3D musical expression in AR.\nAudiences can see how a NIME (new interface for musical expression) in AR\nactivates mobility, space and sound by the performer in musical performance.",
    "authors": "Yichen Wang|Charles Martin",
    "bio": "#### Yichen Wang\n\nYichen Wang is a PhD candidate in computer science at The Australian National\nUniversity, where she explores the relationship between HCI, art and augmented\nreality. Her recent works focus on augmented reality new interfaces for musical\nexpression. You can check out her previous works here:\n<https://yichenwangs.github.io/yichen/work>.\n\n#### Charles Martin\n\nCharles Martin is a computer scientist specialising in music technology, musical\nAI and human-computer interaction at The Australian National University,\nCanberra. Charles develops musical apps such as MicroJam, and PhaseRings,\nresearches creative AI, and performs music with Ensemble Metatone and Andromeda\nis Coming. At the ANU, Charles teaches creative computing and leads research\ninto intelligent musical instruments. His lab's focus is on developing new\nintelligent instruments, performing new music with them, and bringing them to a\nbroad audience of musicians and performers.",
    "image": "TRUE",
    "keywords": "",
    "paper_url": "",
    "session_name": "gig",
    "session_position": "",
    "slides": "",
    "title": "Cubing Sound Ensemble",
    "video": ""
  },
  {
    "UID": "6",
    "abstract": "This installation allows participants to experiment with \u2018fooling\u2019 facial detection models andencourages participants to reflect on the limits of extant surveillance technologies. Physicaladversarial attacks \u2018fool\u2019 such models into labelling a face as \u2018unknown\u2019 or ignoring a facealtogether, using physical props to alter the visual input to a model. Try out one of the propsto conduct your own adversarial attack on a facial detection model!",
    "authors": "Glen Berman|Ned Cooper",
    "bio": "Hi! We are Glen and Ned. We are both PhD candidates at the College of Engineering andComputer Science, Australian National University. Glen studies the social implications ofemerging technology practices, and Ned studies the participation of end users in developingAI-enabled systems.",
    "image": "TRUE",
    "keywords": "",
    "paper_url": "",
    "session_name": "room",
    "session_position": "",
    "slides": "",
    "title": "Machine Masquerade: An Interactive Explorationof the Limits of Machine Recognition",
    "video": ""
  }
]
